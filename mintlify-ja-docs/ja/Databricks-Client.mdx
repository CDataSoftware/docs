---
title: "Databricks"
description: "This page outlines the steps to configure Databricks to access Connect AI. Databricks can pull data from sources that you have connected to your Connect AI account."
---


## Prerequisites

Before you can configure and use Databricks with Connect AI, you must first connect a data source to your Connect AI account. See [Sources](/ja/Sources) for more information.

You must also generate a Personal Access Token (PAT) on the [Settings](/ja/Settings#access-tokens) page. Copy this down, as it acts as your password during authentication.

Make a note of the version of Databricks you are using. Versions of Databricks greater than Runtime 14.3 LTS have different instructions for installing the CData JDBC library.

## Install the JDBC Driver (For All Versions of Databricks)

<Steps>
<Step>
Download and install the Connect AI JDBC driver.

	a. Open the **Integrations** page of Connect AI.

	b. Search for **JDBC** or **Databricks**.

	c. Click **Download** and select your operating system.

	d. When the download is complete, run the setup file.

	e. When the installation is complete, the JAR file can be found in the installation directory.
</Step>
<Step>
Log in to Databricks.
</Step>
</Steps>

## Install the JDBC Library into Databricks (For Versions of Databricks Runtime 14.3 LTS and Below)

<Steps>
<Step>
In the navigation pane, select **Compute**. Start any compute or create a new one.

<Frame>
![](/images/databricks_client_compute.png)
</Frame>
</Step>
<Step>
Once the compute is started, click the compute and then select the **Libraries** tab.

<Frame>
![](/images/databricks_client_libraries.png)
</Frame>
</Step>
<Step>
Click **Install new**. The **Install library** dialog appears.

<Frame>
![](/images/databricks_client_install.png)
</Frame>
</Step>
<Step>
Select **DBFS**. Then drag and drop the JDBC JAR file into the indicated area. The file has the name `cdata.jdbc.connect.jar`. Click **Install**.
</Step>
</Steps>

## Install the JDBC Library into Databricks (For Versions of Databricks Runtime Above 14.3 LTS)

<Steps>
<Step>
In Databricks, create a Catalog named **JARs**.
</Step>
<Step>
Within the **JARs** catalog, create a Schema named **JarFile**.
</Step>
<Step>
In the Catalog Explorer, click **+** and select **Upload to volume**.

<Frame>
![](/images/Databricks-Client.png)
</Frame>
</Step>
<Step>
Click **+** to create a new volume. Name the volume and click **Create**.

<Frame>
![](/images/databricks_client_createvolume.png)
</Frame>
</Step>
<Step>
In the **Upload files to volume** dialog, click **browse**. Upload the Connect JDBC JAR file, named `cdata.jdbc.connect.jar`.
</Step>
<Step>
Install the new library. For **Library Source**, select **Volumes**. Select the JAR file and click **Install**.

<Frame>
![](/images/databricks_client_libraries_volumes.png)
</Frame>

<Note>
**Note:** In order to install the library, you must first add it to the Databricks `allowlist`. See the [Databricks documentation](https://docs.databricks.com/aws/en/data-governance/unity-catalog/manage-privileges/allowlist#how-to-add-items-to-the-allowlist) for more information.
</Note>
</Step>
</Steps>

## Connect Databricks to Connect AI

Once you install the JDBC library, you can connect to Connect AI by running three notebook scripts, one by one.

<Steps>
<Step>
The first script is below. Change the following:
	- Update `User` with your Connect AI username.
	- Update `Password` with the PAT you generated in the prerequisites.
	- Update `Your_Connection_name` with the name of the data source you created in the prerequisites.

```python
driver = "cdata.jdbc.connect.ConnectDriver"
url ="jdbc:connect:AuthScheme=Basic;User=user@cdata.com;Password=***********;URL=https://cloud.cdata.com/api/;DefaultCatalog= Your_Connection_Name;"
```
</Step>
<Step>
Run the first script.
</Step>
<Step>
From the menu on the right side, select **Add cell below** to add a second script. The second script is below. Change the following:
	- Update `User` with your Connect AI username.
	- Update `Password` with the PAT you generated in the prerequisites.
	- Update `Your_Connection_name` with the name of the data source you created in the prerequisites.
	- Update `YOUR_SCHEMA.YOUR_TABLE` with your schema and table, for example, `PUBLIC.CUSTOMERS`.

```python
remote_table = spark.read.format ( "jdbc" ) \
.option ( "driver" , "cdata.jdbc.connect.ConnectDriver") \
.option ( "url","jdbc:connect:AuthScheme=Basic;User=user@cdata.com;Password=*******;URL=https://cloud.cdata.com/api/;DefaultCatalog= Your_Connection_Name;") \
.option ( "dbtable" , "YOUR_SCHEMA.YOUR_TABLE") \
.load ()
```
</Step>
<Step>
Run the second script.
</Step>
<Step>
Add a cell for the third script. The third script is below. Select the columns you want to display.

```python
display (remote_table.select ("ColumnName1","ColumnName2"))
```
</Step>
<Step>
Run the third script.
</Step>
<Step>
You can preview your data in Databricks.
<Columns cols={2}>
<Frame>
![](/images/databricks_client_preview.png)
</Frame>
</Columns>
</Step>
</Steps>